{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0057cedf-ee16-49ea-837e-28d133e65418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e5e17-2c15-4043-b6e8-0c63a9c22678",
   "metadata": {},
   "source": [
    "**Comments:** In this unit, we will use Keras to manage images and create the three sets. We will load all images with this function. We use an image generator with augmentations, such as horizontal flips and rotation range, to introduce more diversity into the training set. As we already have a separate validation dataset, we do not include any validation split in the generator. The aim of data augmentation is to introduce more diversity within the data set pictures by increasing or modifing the size or orientation of pictures. Consequantly, the model uses diversified data to train which helps to generalize better. It is particularly usefl as out dataset is relatively small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150cf640-76bb-477d-b336-f67dc20d3f5e",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c967cef1-ab0d-4b08-98da-c9ddebda0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create image generator with augmentation such as horizontal flip and rotation range to bring more diversity to the train set\n",
    "train_generator = ImageDataGenerator(rescale= 1 / 255, horizontal_flip=True, rotation_range=5)\n",
    "\n",
    "# Create the validation generator \n",
    "val_generator = ImageDataGenerator(rescale= 1/ 255)\n",
    "\n",
    "# Create the test generator\n",
    "test_generator = ImageDataGenerator(rescale= 1/ 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9b0a9e-6162-4fc0-b592-747ffdcbf05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure du dossier pour le volume OS\n",
      "Le numéro de série du volume est A405-9E50\n",
      "C:\\USERS\\SAULAIS\\ONEDRIVE - OCP SA\\BUREAU\\EPFL\\ADSML_COURS_4\\COURSE PROJECT\\CIFAR10\n",
      "Chemin d’accès non valide - \\USERS\\SAULAIS\\ONEDRIVE - OCP SA\\BUREAU\\EPFL\\ADSML_COURS_4\\COURSE PROJECT\\CIFAR10\n",
      "Aucun sous-dossier existant \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tree /A cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771a7560-d78a-4f65-ad9a-20bb5b606deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder list\n",
    "folder_list = [\"bike\", \"car\", \"motorcycle\", \"other\", \"truck\", \"van\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df2b30-9b56-4318-abe8-687d550b6e41",
   "metadata": {},
   "source": [
    "**Comments:** We do not shuffle data for the validation nor test set. In addition, we do not include subset as we have a separate set of data. We keep the default parameter. We keep the original picture size as target size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284a8bbf-cba3-45d3-ba0f-8f4938bb05b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 6 classes.\n",
      "Found 139 images belonging to 6 classes.\n",
      "Found 52 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Train set \n",
    "trainset = train_generator.flow_from_directory(\n",
    "    os.path.join(\"train\"), \n",
    "    classes=folder_list,       \n",
    "    batch_size=32, # We select 32 as a batch size\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=True,\n",
    "    target_size=(224, 224),\n",
    ")\n",
    "\n",
    "# validation and test sets\n",
    "valset = val_generator.flow_from_directory(\n",
    "    os.path.join(\"valid\"), \n",
    "    classes=folder_list, \n",
    "    batch_size=32,\n",
    "    class_mode=\"sparse\",\n",
    "    target_size=(224, 224),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "testset = test_generator.flow_from_directory(\n",
    "    os.path.join(\"test\"),  \n",
    "    classes=folder_list,     \n",
    "    batch_size=32,\n",
    "    target_size=(224, 224),\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece1947e-3135-4802-9bac-fffe0e5493fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((280,), (139,), (52,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels for train, validation, and test sets\n",
    "train_labels = trainset.classes\n",
    "val_labels = valset.classes\n",
    "test_labels = testset.classes\n",
    "train_labels.shape, val_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6f647c-d20e-4cf5-810d-6f09eecbd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def save_h5(iterator, output_path):\n",
    "\n",
    "    images, labels = [], []\n",
    "    \n",
    "    # Loop through the iterator to extract all data\n",
    "    for batch_images, batch_labels in iterator:\n",
    "        images.append(batch_images)\n",
    "        labels.append(batch_labels)\n",
    "        if len(images) * iterator.batch_size >= iterator.n:\n",
    "            break\n",
    "\n",
    "    # Combine batches into single arrays\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    # Save to an HDF5 file\n",
    "    with h5py.File(output_path, \"w\") as h5_file:\n",
    "        h5_file.create_dataset(\"features\", data=images)\n",
    "        h5_file.create_dataset(\"labels\", data=labels)\n",
    "\n",
    "    print(f\"Saved to {output_path}: Features Shape {images.shape}, Labels Shape {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d3d5db-b948-45cb-a08b-2c3231e18bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to train_data_no_hlf.h5: Features Shape (280, 224, 224, 3), Labels Shape (280,)\n",
      "Saved to val_data_no_hlf.h5: Features Shape (139, 224, 224, 3), Labels Shape (139,)\n",
      "Saved to test_data_no_hlf.h5: Features Shape (52, 224, 224, 3), Labels Shape (52, 6)\n"
     ]
    }
   ],
   "source": [
    "# Run the function to train, validation, and test sets to separate .h5 files\n",
    "save_h5(trainset, \"train_data_no_hlf.h5\")\n",
    "save_h5(valset, \"val_data_no_hlf.h5\")\n",
    "save_h5(testset, \"test_data_no_hlf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a3eeb65-bc42-4480-ac64-8ca9e8bf0832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure du dossier pour le volume OS\n",
      "Le numéro de série du volume est A405-9E50\n",
      "C:\\USERS\\SAULAIS\\ONEDRIVE - OCP SA\\BUREAU\\EPFL\\ADSML_COURS_4\\COURSE PROJECT\\TRAIN\n",
      "+---.ipynb_checkpoints\n",
      "+---bike\n",
      "+---car\n",
      "+---motorcycle\n",
      "+---other\n",
      "+---truck\n",
      "\\---van\n"
     ]
    }
   ],
   "source": [
    "# Checking the structure of folders\n",
    "!tree /A train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1622423c-64f8-4b5d-a244-0448f1f481a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), set())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the sets intersets\n",
    "set(trainset.filenames).intersection(set(valset.filenames)), set(trainset.filenames).intersection(set(testset.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de911bd8-f061-49f4-ba87-efba4a81a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images: (32, 224, 224, 3)\n",
      "Batch labels: (32,)\n"
     ]
    }
   ],
   "source": [
    "batch_imgs, batch_labels = trainset.next()\n",
    "\n",
    "print(\"Batch images:\", batch_imgs.shape)\n",
    "print(\"Batch labels:\", batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd1055c8-8d47-4821-91fd-f2e850be3347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bike': 0, 'car': 1, 'motorcycle': 2, 'other': 3, 'truck': 4, 'van': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.class_indices\n",
    "testset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09ebc0-b142-464b-88ca-ea2ec445eab4",
   "metadata": {},
   "source": [
    "*Comments:* The sets do not intersect. Each batch of images has 32 image with an RGB format in 224x224. There are 6 classes in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62464ef3-f659-4334-826b-c05a89327d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the image feature extractor\n",
    "model_url = \"https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/100-224-feature-vector/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e5041a-ddec-4c83-b088-aecc80bedfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = hub.load(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83705169-ace4-426f-99ba-6f08c668b926",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: C:\\Users\\Saulais\\AppData\\Local\\Temp\\tfhub_modules\\145bb06ec3b59b08fb564ab752bd5aa222bfb50a\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12316\\1668023997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Load the pretrained feature extractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfeature_extractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_is_hub_module_v1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Expected before TF2.4.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[0;32m    105\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    862\u001b[0m   \"\"\"\n\u001b[0;32m    863\u001b[0m   \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementReadApi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LOAD_V2_LABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m   \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m   saved_model_proto, debug_info = (\n\u001b[1;32m--> 878\u001b[1;33m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m   \"\"\"\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\adsml\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    116\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     raise IOError(\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: C:\\Users\\Saulais\\AppData\\Local\\Temp\\tfhub_modules\\145bb06ec3b59b08fb564ab752bd5aa222bfb50a\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# URL for MobileNetV2 feature extractor on TensorFlow Hub\n",
    "model_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "\n",
    "# Load the pretrained feature extractor\n",
    "feature_extractor = hub.KerasLayer(model_url, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982a428-abea-4254-bc7b-45a73c5a0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hlf = []\n",
    "train_labels = []\n",
    "test_hlf = []\n",
    "test_labels = []\n",
    "val_hlf = []\n",
    "val_labels = []\n",
    "\n",
    "for _ in range(len(trainset)):  # Number of batches in the dataset\n",
    "    batch_imgs, batch_labels = trainset.next()\n",
    "    batch_features = feature_extractor(batch_imgs)  # Extract features\n",
    "    train_hlf.append(batch_features.numpy())\n",
    "    train_labels.append(batch_labels)\n",
    "for _ in range(len(testset)):  # Number of batches in the dataset\n",
    "    batch_imgs, batch_labels = testset.next()\n",
    "    batch_features = feature_extractor(batch_imgs)  # Extract features\n",
    "    test_hlf.append(batch_features.numpy())\n",
    "    test_labels.append(batch_labels)\n",
    "\n",
    "for _ in range(len(valset)):  # Number of batches in the dataset\n",
    "    batch_imgs, batch_labels = valset.next()\n",
    "    batch_features = feature_extractor(batch_imgs)  # Extract features\n",
    "    val_hlf.append(batch_features.numpy())\n",
    "    val_labels.append(batch_labels)   \n",
    "\n",
    "    \n",
    "# Convert to numpy arrays\n",
    "train_hlf = np.concatenate(train_hlf, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_hlf = np.concatenate(test_hlf, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "val_hlf = np.concatenate(val_hlf, axis=0)\n",
    "val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "print(\"train features shape:\", train_hlf.shape)\n",
    "print(\"train labels shape:\", train_labels.shape)\n",
    "print(\"train features shape:\", val_hlf.shape)\n",
    "print(\"train labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e2e3c-c55b-477b-8b66-5eee58ae92aa",
   "metadata": {},
   "source": [
    "# Model 1 : 1 layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e709248-d8b4-4295-83a5-cdccafb21da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "# Creation of the model\n",
    "model = keras.Sequential()\n",
    "# Adding 1 layer in the model\n",
    "model.add(keras.layers.Dense(units=6, input_dim=1280, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6af5df-03af-4a28-b69e-5f97558a96a4",
   "metadata": {},
   "source": [
    "**Comments:** This 1 layer model includes the 1280 dimensions corresponding to the high level features and the 6 classes. The number of parameters calculated is 7686. We use the softmax activation function as the model is multi classes. We are now compiling the model with a cross entropy loss function and Adam optimizer. We will measure the accuracy to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0d04f-70da-4fbd-9860-523950e41919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a71e3-079a-4193-8549-a116cca2aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End training when accuracy stops improving (optional)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c9b8f-77fe-450b-9339-5d2253ff1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_hlf, train_labels, validation_data=(val_hlf, val_labels), epochs=50, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df712a-4d8c-4251-bbc3-0d08d05b4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss values\n",
    "ax1.set_title(\"loss: {:.4f}\".format(history.history[\"val_loss\"][-1]))\n",
    "ax1.plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "ax1.plot(history.history[\"loss\"], label=\"training\")\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.set_title(\"accuracy: {:.2f}%\".format(history.history[\"val_acc\"][-1] * 100))\n",
    "ax2.plot(history.history[\"val_acc\"], label=\"validation\")\n",
    "ax2.plot(history.history[\"acc\"], label=\"training\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f75230-49b3-463f-91a4-d04cfd6540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_hlf)\n",
    "print(\"Predictions:\", test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd869c11-15db-482b-a767-7b777a3ed6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(\n",
    "    y_true=testset.classes,            # array with true labels\n",
    "    y_pred=test_preds.argmax(axis=1),  # array with predicted labels\n",
    ")\n",
    "\n",
    "# Format as a DataFrame\n",
    "class_names = list(testset.class_indices.keys())\n",
    "matrix_df = pd.DataFrame(data=matrix, columns=class_names, index=class_names)\n",
    "matrix_df.columns.name = \"Predictions\"\n",
    "matrix_df.index.name = \"True class\"\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdc657-6a1d-45b8-a77c-f33155395e92",
   "metadata": {},
   "source": [
    "# As test data are one hot encoded, we have to convert them back into numbers from 0 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98b78c-e7f1-4738-8ac3-a5826e253426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels\n",
    "test_labels_converted = np.argmax(test_labels, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a4688-2a8f-4c3c-9287-ff89df502052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model \n",
    "(test_loss, nn_score) = model.evaluate(test_hlf, test_labels_converted, batch_size=32)\n",
    "nn_score_1 = nn_score * 100\n",
    "print(\"Test loss: {:.2f}\".format(test_loss))\n",
    "print(\"Test accuracy: {:.2f}%\".format(nn_score_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33c4e0-aa32-4e8d-99e9-f011f4c05e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the score in the SQL Table \n",
    "import sqlite3\n",
    "\n",
    "layer_1_data = {'model': ['1-layer-nn'],\n",
    "        'test_accuracy': [nn_score_1]}\n",
    "layer_1_df = pd.DataFrame(layer_1_data)\n",
    "\n",
    "# Save to SQLite\n",
    "conn = sqlite3.connect('results.db')\n",
    "layer_1_df.to_sql('results_table', conn, if_exists='append', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c55e14-19ee-402d-b182-a675b91709ba",
   "metadata": {},
   "source": [
    "*Comment:* This first model is already good with 90% of accuracy. Based on the test set, the model made 1 error classifying a van as a car. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdc141-ee7b-42da-8eaf-a73c9b6f5745",
   "metadata": {},
   "source": [
    "# Model 2 : 2 Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd0181-8993-42b2-80df-9c8baeb1469c",
   "metadata": {},
   "source": [
    "*Comment:* We are now adding a new layer with Relu activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8edc5-1056-458d-b261-f943182faeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the model\n",
    "model_2 = keras.Sequential()\n",
    "# Adding 1 layer in the model with activation function relu\n",
    "model_2.add(keras.layers.Dense(units=128, input_dim=1280, activation=\"relu\")),\n",
    "\n",
    "model_2.add(keras.layers.Dense(units=6, activation=\"softmax\")),\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5dac6-d4ac-4c8d-a3ac-2fbd04fb95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d95b8e-7459-4834-aa6b-d63528b3962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_2 = model_2.fit(train_hlf, train_labels, validation_data=(val_hlf, val_labels), epochs=50, callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851b7df-f0c5-46e9-bdd8-c41b1be2e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss values\n",
    "ax1.set_title(\"loss: {:.4f}\".format(history_2.history[\"val_loss\"][-1]))\n",
    "ax1.plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "ax1.plot(history.history[\"loss\"], label=\"training\")\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.set_title(\"accuracy: {:.2f}%\".format(history_2.history[\"val_acc\"][-1] * 100))\n",
    "ax2.plot(history.history[\"val_acc\"], label=\"validation\")\n",
    "ax2.plot(history.history[\"acc\"], label=\"training\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e2296-8867-4c5a-ae6b-ac33e18f982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_2 = model_2.predict(test_hlf)\n",
    "print(\"Predictions:\", test_preds_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0391f-6e57-49ca-a747-258dc0f0e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(\n",
    "    y_true=testset.classes,            # array with true labels\n",
    "    y_pred=test_preds_2.argmax(axis=1),  # array with predicted labels\n",
    ")\n",
    "\n",
    "# Format as a DataFrame\n",
    "class_names = list(testset.class_indices.keys())\n",
    "matrix_df = pd.DataFrame(data=matrix, columns=class_names, index=class_names)\n",
    "matrix_df.columns.name = \"Predictions\"\n",
    "matrix_df.index.name = \"True class\"\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6ce02-5a73-427d-b3f4-74c0c20c99c0",
   "metadata": {},
   "source": [
    "**Comments:** We managed toimprove the accuracy of the model slightly in adding an additional layer. This model made only 1 mistake classifying a van as a car. We can say that adding a second layer doesn't improve the performance of the model. It is likely due to the fact that we are using high level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe82337-d58d-434e-9d7c-f3f1e8404d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model \n",
    "(test_loss, nn_2_score) = model_2.evaluate(test_hlf, test_labels_converted, batch_size=32)\n",
    "nn_score_2 = nn_2_score * 100\n",
    "print(\"Test loss: {:.2f}\".format(test_loss))\n",
    "print(\"Test accuracy: {:.2f}%\".format(100 * nn_score_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79859fe-b9b1-4db5-8d03-803ff035b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the score in the SQL Table \n",
    "import sqlite3\n",
    "\n",
    "layer_2_data = {'model': ['2-layer-nn'],\n",
    "        'test_accuracy': [nn_score_2]}\n",
    "layer_2_df = pd.DataFrame(layer_2_data)\n",
    "\n",
    "# Save to SQLite\n",
    "conn = sqlite3.connect('results.db')\n",
    "layer_2_df.to_sql('results_table', conn, if_exists='append', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12f113-ed69-4712-9cd1-4fd4d6ed4177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml] *",
   "language": "python",
   "name": "conda-env-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
